\documentclass{article}
\usepackage{mathpazo}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{url,booktabs}
\begin{document}

\title{Hippocampus vs Cortex Project: classification}


\author{EC, SJE}

\date{\today}
\maketitle


\section*{Setup}

<<setup-knitr,eval=TRUE,include=FALSE>>=
library(knitr)
library(xtable)
options(width=60)
opts_chunk$set(cache=TRUE)
opts_chunk$set(echo=TRUE)
opts_chunk$set(timeit=FALSE)
opts_chunk$set(dev='pdf')
@


<<setup-timeit, include=FALSE,timeit=FALSE>>=
knitr::knit_hooks$set(timeit = local({
  now = NULL
  function(before, options, envir) {
    if (before) {
      now <<- Sys.time()
    } else {
      if (is.null(now)) {
        sprintf('\nnow is null in chunk %s\n', options$label)
      } else {
        res = difftime(Sys.time(), now)
        now <<- NULL
        ## use options$label if you want the chunk label as well
        sprintf('\nTime for this code chunk %.3f s',
                as.numeric(as.character(res)))
      }
    }
  }})
)
@

<<external-packages,include=TRUE,results='hide'>>=
library(sjemea)                         #my package
library(rhdf5)
library(parallel)
library(g2chvcdata)
library(g2chvc)
## Following for classifiers
library(tree)
library(randomForest)
library(gbm)
library(e1071)
library(modeest)
@ 

\section*{Classification}

Run the tree and SVM classifiers.  Taken from classification.R

<<classification-startup>>=
##Set cores from the command line is easier.
##options(mc.cores=12)                     #set  to number of cores you have
data.df <- read.csv(system.file("stored/hvcfeatures.csv", package="g2chvc"),
                    row.names=1)
ages<-c(7, 10, 11, 14, 17, 18, 21, 24, 25, 28)
regions<-c("ctx", "hpc")
print(getOption("mc.cores"))
@ 



<<class-decision,timeit=TRUE>>=
##Decision trees classification results
## SJE: temp vvv
## tree.all<-boost.tree.all(data.df) #Results for classification into combined age and region
##
## SJE: temp vvv
##Results for predicting age for ctx and hpc separately
##tree.region<-mclapply(regions, function(x) {
##  boost.tree.region(data.df, x, nreps=100)
##})
##names(tree.region)<-regions
## Finally, tree.age stores results predicting region for each DIV separately
tree.age<-mclapply(ages, function(x) {
  boost.tree.age(data.df, x,nreps=250)
})
names(tree.age)<-ages
@


<<class-decision-feature-list>>=
#Most important factors, by Mean Decrease in Gini.
feature.mat<-sapply(tree.age, "[[", 2)
order.mat<-apply(feature.mat,2, order) #Each column is DIV, each row is feature number in order of increasing importance
avg.order<-order(apply(feature.mat, 1, mean))#Order of importance of factors, averaged over all ages
@ 

<<show-order-mat>>=
print(order.mat)
@ 

<<class-sje-check-ranking>>=
scores <- apply(feature.mat, 1, mean)
print(sort(scores))
@ 

\section{SVM results}

Uses training set of 2/3 of data and remaining as test set To use
leave one out cross validation instead, change "test.set" to "LOOCV"


<<class-svn,timeit=TRUE>>=
test.method <- "test.set"                  # should be "LOOCV" or "test.set"
#Accuracy of SVMs using linear and radial kernels and all 11 features
#lin.MSE<-mclapply(ages, function(x) svm.calc(data.df, x, "linear", test.method))
rad.MSE<-mclapply(ages, function(x) svm.calc(data.df, x, "radial", test.method))
@ 


\subsection*{Reducing the number of features to best N, where N=1 to 11}

Accuracy of models with  1--11 features using average important
features and radial kernel.   To test the case N=11 features, we
simply set feat.rm = 0.


<<class-svn-reduced,timeit=TRUE>>=
feat.rm<-10:0
avg.features<-mclapply(feat.rm, function(x)
                       avg.feature.rm(data.df, avg.order, x,
                                      "radial", test.method))
names(avg.features)<- paste(11-feat.rm, "features")
@ 

<<latex-names-for-features>>=
latex.names <- new.env()
assign("CV.IBI", "CV of IBI", latex.names)
assign("corr", "mean correlation", latex.names)
assign("burst.rate", "burst rate", latex.names)
assign("burst.dur", "burst duration", latex.names)
assign("spikes.in.bursts", "% of spikes in bursts", latex.names)
assign("fir.rate", "firing rate", latex.names)
assign("burst.fir.rate", "w/in burst firing rate", latex.names)
assign("n.s.peak", "NS peak", latex.names)
assign("n.s.dur", "NS duration", latex.names)
assign("num.n.s", "NS rate", latex.names)
assign("theta.burst", "theta burst", latex.names)
@ 

<<print-reduced-features-performance,timeit=FALSE>>=
m <- signif(do.call("rbind", avg.features) * 100, 3)
colnames(m) <- ages
features.bestfirst <- names(sort(-scores))
features.bestfirst.l <- sapply(features.bestfirst, get, env=latex.names)
sorted.scores <- (sort(-scores) * -1)
sorted.scores <- signif(sorted.scores / sorted.scores[1], 2) #normalise to 1.00

m.df <- data.frame(features.bestfirst.l, sorted.scores, m)
names(m.df) <- c("Feature", "Score", ages)
digits <- c(0,0,2,rep(0,10))
xtab <- xtable(m.df,caption="SVM performance",
               label="tab:svm",
               digits=digits)
@ 

<<reduced-features-table,results='asis',timeit=FALSE>>=
print(xtab, booktabs=TRUE,include.rownames=FALSE)
print(xtab, booktabs=TRUE,include.rownames=FALSE,
      file="/tmp/svm-reduced-features.tex")
@ 

Finally, we can individually train the SVM and find best features.
individually at each age.

Report MSE of models with between varying number of features using
individual important features for each DIV.

<<svm-individual-age-optim-features,timeit=TRUE>>=
indiv.features<-mclapply(feat.rm, function(x)
                         feature.rm(data.df, order.mat, x, "radial",
                                    test.type=test.method))
names(indiv.features)<- paste(11-feat.rm, "features")
@

<<optime-print-reduced-features-performance>>=
optim.m <- do.call("rbind", indiv.features)
colnames(optim.m) <- ages
@ 

<<optim-reduced-features-table,results='asis'>>=
print(xtable(optim.m))
@ 

\section*{About this document}

<<eval=FALSE>>=
knitr::knit2pdf("hvc_class.Rnw")
@ 

\end{document}
